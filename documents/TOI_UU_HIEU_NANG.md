# üöÄ H∆Ø·ªöNG D·∫™N T·ªêI ∆ØU HI·ªÜU NƒÇNG H·ªÜ TH·ªêNG SHOPEE CLONE

## üìã M·ª§C L·ª§C

1. [T·ªïng Quan V·∫•n ƒê·ªÅ](#1-t·ªïng-quan-v·∫•n-ƒë·ªÅ)
2. [T·ªëi ∆Øu Kafka Consumers](#2-t·ªëi-∆∞u-kafka-consumers)
3. [T·ªëi ∆Øu Database Connection Pool](#3-t·ªëi-∆∞u-database-connection-pool)
4. [T·ªëi ∆Øu Application Server Thread Pool](#4-t·ªëi-∆∞u-application-server-thread-pool)
5. [T·ªëi ∆Øu WebSocket Configuration](#5-t·ªëi-∆∞u-websocket-configuration)
6. [C·∫•u H√¨nh MySQL](#6-c·∫•u-h√¨nh-mysql)
7. [Monitoring & Metrics](#7-monitoring--metrics)
8. [Testing & Verification](#8-testing--verification)
9. [Checklist T·ªëi ∆Øu](#9-checklist-t·ªëi-∆∞u)

---

## 1. T·ªîNG QUAN V·∫§N ƒê·ªÄ

### 1.1. C√°c Bottleneck Hi·ªán T·∫°i

| Component | V·∫•n ƒê·ªÅ | Impact | Priority |
|-----------|--------|--------|----------|
| **Notification Service Kafka** | `concurrency = 1` (1 thread cho 10 partitions) | ‚ö†Ô∏è **CRITICAL** | üî¥ **HIGH** |
| **Order Service Kafka** | `concurrency = 1` (m·∫∑c ƒë·ªãnh) | ‚ö†Ô∏è **HIGH** | üü° **MEDIUM** |
| **Database Pool** | `max-pool-size = 10` (m·∫∑c ƒë·ªãnh) | ‚ö†Ô∏è **HIGH** | üü° **MEDIUM** |
| **Tomcat Threads** | `max-threads = 200` (m·∫∑c ƒë·ªãnh) | ‚ö†Ô∏è **MEDIUM** | üü¢ **LOW** |
| **WebSocket** | In-memory broker (kh√¥ng scale) | ‚ö†Ô∏è **LOW** | üü¢ **LOW** |

### 1.2. Capacity Hi·ªán T·∫°i vs Sau T·ªëi ∆Øu

| Metric | Hi·ªán T·∫°i | Sau T·ªëi ∆Øu | C·∫£i Thi·ªán |
|--------|----------|------------|-----------|
| **Concurrent Users** | ~1,000 | ~5,000-10,000 | **5-10x** |
| **Notifications/sec** | ~10-15 | ~100-150 | **10x** |
| **Orders/sec** | ~2-3 | ~20-30 | **10x** |
| **Database Queries/sec** | ~100-150 | ~500-1000 | **5-7x** |
| **HTTP Requests/sec** | ~1,000-1,500 | ~5,000-10,000 | **5-7x** |

---

## 2. T·ªêI ∆ØU KAFKA CONSUMERS

### 2.1. V·∫•n ƒê·ªÅ Hi·ªán T·∫°i

**Notification Service:**
```java
// notification-service/src/main/java/.../KafkaConfig.java
factory.setConcurrency(1); // ‚ùå 1 thread x·ª≠ l√Ω 10 partitions ‚Üí R·∫§T CH·∫¨M
```

**V·∫•n ƒë·ªÅ:**
- 1 thread ph·∫£i x·ª≠ l√Ω 10 partitions ‚Üí sequential processing
- Throughput: ~10-15 notifications/gi√¢y
- Latency cao khi c√≥ nhi·ªÅu notifications

**Gi·∫£i th√≠ch:**
- Kafka partition = ƒë∆°n v·ªã song song h√≥a
- M·ªói partition ch·ªâ c√≥ th·ªÉ ƒë∆∞·ª£c consume b·ªüi 1 consumer trong c√πng consumer group
- `concurrency = 1` ‚Üí ch·ªâ 1 thread consume t·∫•t c·∫£ partitions ‚Üí kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c parallelism

### 2.2. Gi·∫£i Ph√°p: TƒÉng Concurrency

**Rule of thumb:**
- `concurrency` ‚â§ s·ªë partitions
- T·ªët nh·∫•t: `concurrency = s·ªë partitions` (1 thread per partition)

**Fix cho Notification Service:**

**File:** `notification-service/src/main/java/com/example/notificationservice/config/KafkaConfig.java`

```java
package com.example.notificationservice.config;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.support.serializer.JsonDeserializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaConfig {

    @Value("${kafka.topic.notification}")
    private String notificationTopic;

    @Value("${spring.kafka.consumer.bootstrap-servers}")
    private String bootstrapServers;

    @Value("${spring.kafka.consumer.group-id}")
    private String groupId;

    @Bean
    public ConsumerFactory<String, SendNotificationRequest> consumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);

        // Enable auto-commit
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000);

        // JsonDeserializer config
        props.put(JsonDeserializer.VALUE_DEFAULT_TYPE, SendNotificationRequest.class);
        props.put(JsonDeserializer.USE_TYPE_INFO_HEADERS, false);
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        
        // ‚úÖ T·ªêI ∆ØU: TƒÉng fetch size v√† batch size ƒë·ªÉ gi·∫£m s·ªë l·∫ßn fetch
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024); // Fetch khi c√≥ √≠t nh·∫•t 1KB
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500); // ƒê·ª£i t·ªëi ƒëa 500ms
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500); // Poll t·ªëi ƒëa 500 records m·ªói l·∫ßn
        
        return new DefaultKafkaConsumerFactory<>(props);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, SendNotificationRequest> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, SendNotificationRequest> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());

        // ‚úÖ T·ªêI ∆ØU: 10 threads cho 10 partitions (1 thread per partition)
        // ƒê·∫£m b·∫£o ordering: messages trong c√πng partition v·∫´n ƒë∆∞·ª£c x·ª≠ l√Ω tu·∫ßn t·ª±
        factory.setConcurrency(10);

        // ‚úÖ T·ªêI ∆ØU: Enable batch processing n·∫øu c·∫ßn x·ª≠ l√Ω nhi·ªÅu messages c√πng l√∫c
        // factory.setBatchListener(true); // Uncomment n·∫øu mu·ªën nh·∫≠n List<SendNotificationRequest>

        return factory;
    }
}
```

**Gi·∫£i th√≠ch:**
- `setConcurrency(10)`: 10 threads, m·ªói thread x·ª≠ l√Ω 1 partition ‚Üí parallel processing
- `FETCH_MIN_BYTES`: Gi·∫£m s·ªë l·∫ßn fetch t·ª´ Kafka
- `MAX_POLL_RECORDS`: TƒÉng s·ªë records poll m·ªói l·∫ßn ‚Üí gi·∫£m overhead

**Impact:**
- Throughput: ~10-15/s ‚Üí **~100-150/s** (10x)
- Latency: Gi·∫£m ƒë√°ng k·ªÉ

---

### 2.3. Fix cho Order Service

**File:** `order-service/src/main/java/com/example/orderservice/config/KafkaConfig.java`

**B∆∞·ªõc 1:** T·∫°o KafkaConfig n·∫øu ch∆∞a c√≥:

```java
package com.example.orderservice.config;

import com.example.orderservice.dto.CheckOutKafkaRequest;
import com.example.orderservice.dto.PaymentEvent;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.support.serializer.JsonDeserializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaConfig {

    @Value("${spring.kafka.consumer.bootstrap-servers}")
    private String bootstrapServers;

    @Value("${spring.kafka.consumer.group-id}")
    private String groupId;

    // ‚úÖ Consumer Factory cho CheckOutKafkaRequest
    @Bean
    public ConsumerFactory<String, CheckOutKafkaRequest> checkoutConsumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        
        props.put(JsonDeserializer.VALUE_DEFAULT_TYPE, CheckOutKafkaRequest.class);
        props.put(JsonDeserializer.USE_TYPE_INFO_HEADERS, false);
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        
        // ‚úÖ T·ªêI ∆ØU
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024);
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);
        
        return new DefaultKafkaConsumerFactory<>(props);
    }

    // ‚úÖ Consumer Factory cho PaymentEvent
    @Bean
    public ConsumerFactory<String, PaymentEvent> paymentConsumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        
        props.put(JsonDeserializer.VALUE_DEFAULT_TYPE, PaymentEvent.class);
        props.put(JsonDeserializer.USE_TYPE_INFO_HEADERS, false);
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        
        // ‚úÖ T·ªêI ∆ØU
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024);
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);
        
        return new DefaultKafkaConsumerFactory<>(props);
    }

    // ‚úÖ Listener Factory cho Checkout
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, CheckOutKafkaRequest> checkoutListenerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, CheckOutKafkaRequest> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(checkoutConsumerFactory());
        factory.setConcurrency(10); // ‚úÖ 10 threads cho 10 partitions
        return factory;
    }

    // ‚úÖ Listener Factory cho Payment
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, PaymentEvent> paymentListenerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, PaymentEvent> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(paymentConsumerFactory());
        factory.setConcurrency(10); // ‚úÖ 10 threads cho 10 partitions
        return factory;
    }
}
```

**B∆∞·ªõc 2:** Update OrderServiceImpl ƒë·ªÉ s·ª≠ d·ª•ng factory m·ªõi:

**File:** `order-service/src/main/java/com/example/orderservice/service/OrderServiceImpl.java`

```java
// Th√™m import
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;

// Update @KafkaListener ƒë·ªÉ ch·ªâ ƒë·ªãnh containerFactory
@KafkaListener(
    topics = "#{@orderTopic.name}", 
    groupId = "order-service-checkout",
    containerFactory = "checkoutListenerFactory" // ‚úÖ Ch·ªâ ƒë·ªãnh factory
)
@Transactional
public void consumeCheckout(CheckOutKafkaRequest msg) {
    // ... existing code ...
}

@KafkaListener(
    topics = "#{@paymentTopic.name}", 
    groupId = "order-service-payment",
    containerFactory = "paymentListenerFactory" // ‚úÖ Ch·ªâ ƒë·ªãnh factory
)
@Transactional
public void consumePaymentEvent(PaymentEvent event) {
    // ... existing code ...
}
```

**Impact:**
- Throughput: ~2-3 orders/s ‚Üí **~20-30 orders/s** (10x)

---

### 2.4. Verify Kafka Consumer Configuration

**C√°ch ki·ªÉm tra:**

1. **Xem s·ªë threads ƒëang ch·∫°y:**
```bash
# V√†o notification-service logs
tail -f logs/notification-service.log | grep "KafkaListenerContainer"

# Ho·∫∑c check JVM threads
jstack <pid> | grep -i kafka
```

2. **Monitor consumer lag:**
```bash
# S·ª≠ d·ª•ng Kafka UI (port 9090)
# Ho·∫∑c kafka-consumer-groups command
kafka-consumer-groups --bootstrap-server localhost:9092 \
  --group notification-service-group \
  --describe
```

3. **Test throughput:**
```java
// T·∫°o test script ƒë·ªÉ g·ª≠i nhi·ªÅu notifications
// ƒêo th·ªùi gian x·ª≠ l√Ω
```

---

## 3. T·ªêI ∆ØU DATABASE CONNECTION POOL

### 3.1. V·∫•n ƒê·ªÅ Hi·ªán T·∫°i

**HikariCP Default:**
- `maximum-pool-size = 10`
- `minimum-idle = 10`
- Kh√¥ng c√≥ timeout configuration

**V·∫•n ƒë·ªÅ:**
- 9 services √ó 10 connections = 90 connections
- MySQL default `max_connections = 151` ‚Üí g·∫ßn ƒë·∫°t gi·ªõi h·∫°n
- Connection pool exhaustion khi c√≥ nhi·ªÅu requests ƒë·ªìng th·ªùi

### 3.2. Gi·∫£i Ph√°p: TƒÉng Pool Size

**File:** `config/application.properties` (ho·∫∑c m·ªói service ri√™ng)

```properties
# ‚úÖ T·ªêI ∆ØU: Database Connection Pool Configuration
# HikariCP l√† connection pool m·∫∑c ƒë·ªãnh c·ªßa Spring Boot (t·ªët nh·∫•t)

# Maximum s·ªë connections trong pool
spring.datasource.hikari.maximum-pool-size=20

# Minimum s·ªë idle connections (lu√¥n gi·ªØ s·∫µn)
spring.datasource.hikari.minimum-idle=5

# Timeout khi ch·ªù connection t·ª´ pool (ms)
spring.datasource.hikari.connection-timeout=30000

# Timeout cho idle connections (ms) - t·ª± ƒë·ªông ƒë√≥ng connections kh√¥ng d√πng
spring.datasource.hikari.idle-timeout=600000

# Maximum lifetime c·ªßa connection (ms) - ƒë√≥ng connection sau th·ªùi gian n√†y
spring.datasource.hikari.max-lifetime=1800000

# Ph√°t hi·ªán connection leak (ms) - c·∫£nh b√°o n·∫øu connection kh√¥ng ƒë∆∞·ª£c ƒë√≥ng
spring.datasource.hikari.leak-detection-threshold=60000

# Connection test query (ƒë·∫£m b·∫£o connection c√≤n s·ªëng)
spring.datasource.hikari.connection-test-query=SELECT 1

# Pool name (ƒë·ªÉ d·ªÖ debug)
spring.datasource.hikari.pool-name=ShopeeHikariPool
```

**Gi·∫£i th√≠ch t·ª´ng tham s·ªë:**
- `maximum-pool-size`: S·ªë connections t·ªëi ƒëa. **Rule:** `(max_connections / s·ªë_services) - 10` (buffer)
- `minimum-idle`: S·ªë connections gi·ªØ s·∫µn ‚Üí gi·∫£m latency khi c√≥ request m·ªõi
- `connection-timeout`: N·∫øu pool h·∫øt connections, ƒë·ª£i t·ªëi ƒëa 30s ‚Üí tr·∫£ v·ªÅ error
- `idle-timeout`: ƒê√≥ng connections kh√¥ng d√πng sau 10 ph√∫t ‚Üí gi·∫£i ph√≥ng t√†i nguy√™n
- `max-lifetime`: ƒê√≥ng connection sau 30 ph√∫t (d√π ƒëang d√πng) ‚Üí tr√°nh stale connections
- `leak-detection-threshold`: C·∫£nh b√°o n·∫øu connection kh√¥ng ƒë∆∞·ª£c ƒë√≥ng sau 60s ‚Üí ph√°t hi·ªán bug

### 3.3. C·∫•u H√¨nh Cho T·ª´ng Service

**Services c√≥ nhi·ªÅu traffic (c·∫ßn pool l·ªõn h∆°n):**
- `order-service`
- `stock-service`
- `user-service`
- `gateway` (n·∫øu c√≥ database)

**File:** `order-service/src/main/resources/application.properties`

```properties
# Order Service - nhi·ªÅu traffic
spring.datasource.hikari.maximum-pool-size=30
spring.datasource.hikari.minimum-idle=10
```

**Services √≠t traffic:**
- `auth-service`
- `file-storage`
- `notification-service` (ch·ªß y·∫øu d√πng Kafka)

**File:** `auth-service/src/main/resources/application.properties`

```properties
# Auth Service - √≠t traffic
spring.datasource.hikari.maximum-pool-size=15
spring.datasource.hikari.minimum-idle=3
```

### 3.4. Monitor Connection Pool

**Th√™m Actuator ƒë·ªÉ monitor:**

**File:** `pom.xml` (m·ªói service)

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```

**File:** `application.properties`

```properties
# Enable HikariCP metrics
management.endpoints.web.exposure.include=health,metrics,hikaricp
management.metrics.export.prometheus.enabled=true
```

**Access metrics:**
```bash
# Health check
curl http://localhost:8005/actuator/health

# HikariCP metrics
curl http://localhost:8005/actuator/metrics/hikaricp.connections.active
curl http://localhost:8005/actuator/metrics/hikaricp.connections.idle
curl http://localhost:8005/actuator/metrics/hikaricp.connections.pending
```

---

## 4. T·ªêI ∆ØU APPLICATION SERVER THREAD POOL

### 4.1. V·∫•n ƒê·ªÅ Hi·ªán T·∫°i

**Tomcat Default:**
- `max-threads = 200`
- `min-spare-threads = 10`
- `accept-count = 100`

**V·∫•n ƒë·ªÅ:**
- Khi c√≥ > 200 requests ƒë·ªìng th·ªùi ‚Üí requests ph·∫£i ƒë·ª£i
- Queue ƒë·∫ßy ‚Üí connection timeout

### 4.2. Gi·∫£i Ph√°p: TƒÉng Thread Pool

**File:** `order-service/src/main/resources/application.properties`

```properties
# ‚úÖ T·ªêI ∆ØU: Tomcat Thread Pool Configuration

# Maximum s·ªë threads x·ª≠ l√Ω requests
server.tomcat.threads.max=500

# Minimum s·ªë threads gi·ªØ s·∫µn (lu√¥n c√≥ s·∫µn)
server.tomcat.threads.min-spare=50

# Maximum s·ªë connections ch·ªù trong queue (khi t·∫•t c·∫£ threads ƒëang busy)
server.tomcat.accept-count=1000

# Maximum s·ªë connections TCP (t·ªïng s·ªë connections)
server.tomcat.max-connections=10000

# Connection timeout (ms) - ƒë√≥ng connection n·∫øu kh√¥ng c√≥ request trong 20s
server.connection-timeout=20000

# Enable compression
server.compression.enabled=true
server.compression.mime-types=text/html,text/xml,text/plain,text/css,text/javascript,application/javascript,application/json
server.compression.min-response-size=1024
```

**Gi·∫£i th√≠ch:**
- `threads.max`: S·ªë threads t·ªëi ƒëa. **Rule:** `(CPU cores √ó 2) + s·ªë_IO_operations`
- `threads.min-spare`: Gi·ªØ s·∫µn threads ‚Üí gi·∫£m latency
- `accept-count`: Queue size ‚Üí ch·ª©a requests khi threads ƒëang busy
- `max-connections`: T·ªïng s·ªë TCP connections ‚Üí ph·∫£i > `threads.max + accept-count`

**C·∫•u h√¨nh theo service:**

**High Traffic Services:**
```properties
# order-service, stock-service, gateway
server.tomcat.threads.max=500
server.tomcat.threads.min-spare=50
server.tomcat.accept-count=1000
server.tomcat.max-connections=10000
```

**Medium Traffic Services:**
```properties
# user-service, notification-service
server.tomcat.threads.max=300
server.tomcat.threads.min-spare=30
server.tomcat.accept-count=500
server.tomcat.max-connections=5000
```

**Low Traffic Services:**
```properties
# auth-service, file-storage
server.tomcat.threads.max=200
server.tomcat.threads.min-spare=20
server.tomcat.accept-count=200
server.tomcat.max-connections=2000
```

### 4.3. Monitor Thread Pool

**Actuator metrics:**
```bash
# Thread pool metrics
curl http://localhost:8005/actuator/metrics/tomcat.threads.busy
curl http://localhost:8005/actuator/metrics/tomcat.threads.current
curl http://localhost:8005/actuator/metrics/tomcat.connections.active
curl http://localhost:8005/actuator/metrics/tomcat.connections.max
```

---

## 5. T·ªêI ∆ØU WEBSOCKET CONFIGURATION

### 5.1. V·∫•n ƒê·ªÅ Hi·ªán T·∫°i

**In-Memory Broker:**
- Kh√¥ng scale ƒë∆∞·ª£c (ch·ªâ trong 1 JVM)
- M·∫•t messages khi service restart
- Kh√¥ng th·ªÉ load balance

### 5.2. Gi·∫£i Ph√°p: External Message Broker (RabbitMQ/Redis)

**Option 1: RabbitMQ (Khuy·∫øn ngh·ªã cho production)**

**File:** `notification-service/pom.xml`

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

**File:** `notification-service/src/main/resources/application.properties`

```properties
# RabbitMQ Configuration
spring.rabbitmq.host=localhost
spring.rabbitmq.port=5672
spring.rabbitmq.username=guest
spring.rabbitmq.password=guest

# STOMP over WebSocket v·ªõi RabbitMQ
spring.websocket.stomp.relay.enabled=true
spring.websocket.stomp.relay.host=localhost
spring.websocket.stomp.relay.port=61613
spring.websocket.stomp.relay.client-login=guest
spring.websocket.stomp.relay.server-login=guest
```

**File:** `notification-service/src/main/java/.../WebSocketConfig.java`

```java
@Configuration
@EnableWebSocketMessageBroker
public class WebSocketConfig implements WebSocketMessageBrokerConfigurer {

    @Override
    public void configureMessageBroker(MessageBrokerRegistry config) {
        // ‚úÖ S·ª≠ d·ª•ng RabbitMQ thay v√¨ in-memory
        config.enableStompBrokerRelay("/topic", "/queue")
            .setRelayHost("localhost")
            .setRelayPort(61613)
            .setClientLogin("guest")
            .setClientPasscode("guest");
        
        config.setApplicationDestinationPrefixes("/app");
    }

    // ... rest of config ...
}
```

**Option 2: Redis (ƒê∆°n gi·∫£n h∆°n, nh∆∞ng √≠t t√≠nh nƒÉng h∆°n)**

**File:** `notification-service/pom.xml`

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-session-data-redis</artifactId>
</dependency>
```

### 5.3. T·ªëi ∆Øu WebSocket Connections

**File:** `notification-service/src/main/resources/application.properties`

```properties
# ‚úÖ T·ªêI ∆ØU: WebSocket Configuration

# Maximum s·ªë WebSocket connections
spring.websocket.max-connections=10000

# Heartbeat interval (gi·ªØ connection alive)
spring.websocket.heartbeat.interval=30000

# Connection timeout
spring.websocket.timeout=3600000
```

**File:** `notification-service/src/main/java/.../WebSocketConfig.java`

```java
@Override
public void configureClientInboundChannel(ChannelRegistration registration) {
    // ‚úÖ TƒÉng thread pool cho WebSocket
    registration.taskExecutor()
        .corePoolSize(10)
        .maxPoolSize(20)
        .queueCapacity(1000);
    
    registration.interceptors(webSocketJwtInterceptor);
}

@Override
public void configureClientOutboundChannel(ChannelRegistration registration) {
    // ‚úÖ TƒÉng thread pool cho outbound messages
    registration.taskExecutor()
        .corePoolSize(10)
        .maxPoolSize(20)
        .queueCapacity(1000);
}
```

---

## 6. C·∫§U H√åNH MYSQL

### 6.1. T·ªëi ∆Øu MySQL Configuration

**File:** `my.cnf` ho·∫∑c `my.ini` (MySQL config file)

```ini
[mysqld]
# ‚úÖ T·ªêI ∆ØU: Connection Settings
max_connections=500
max_user_connections=400

# ‚úÖ T·ªêI ∆ØU: InnoDB Buffer Pool (quan tr·ªçng nh·∫•t!)
# Rule: 70-80% c·ªßa RAM (n·∫øu MySQL l√† service ch√≠nh)
# V√≠ d·ª•: 16GB RAM ‚Üí 8-12GB cho buffer pool
innodb_buffer_pool_size=8G
innodb_buffer_pool_instances=8

# ‚úÖ T·ªêI ∆ØU: InnoDB Log Files
innodb_log_file_size=512M
innodb_log_buffer_size=64M

# ‚úÖ T·ªêI ∆ØU: Query Cache (MySQL 5.7 tr·ªü xu·ªëng)
# query_cache_size=256M
# query_cache_type=1

# ‚úÖ T·ªêI ∆ØU: Table Cache
table_open_cache=4000
table_definition_cache=2000

# ‚úÖ T·ªêI ∆ØU: Thread Settings
thread_cache_size=50
thread_stack=256K

# ‚úÖ T·ªêI ∆ØU: Connection Timeouts
wait_timeout=600
interactive_timeout=600

# ‚úÖ T·ªêI ∆ØU: Slow Query Log
slow_query_log=1
slow_query_log_file=/var/log/mysql/slow-query.log
long_query_time=2

# ‚úÖ T·ªêI ∆ØU: Binary Log (n·∫øu c·∫ßn replication)
# log_bin=/var/log/mysql/mysql-bin.log
# binlog_format=ROW
# expire_logs_days=7
```

**Gi·∫£i th√≠ch:**
- `max_connections`: T·ªïng s·ªë connections. **Rule:** `(s·ªë_services √ó pool_size) + 100` (buffer)
- `innodb_buffer_pool_size`: Cache data v√† indexes ‚Üí gi·∫£m disk I/O. **Quan tr·ªçng nh·∫•t!**
- `innodb_log_file_size`: Transaction log size ‚Üí ·∫£nh h∆∞·ªüng ƒë·∫øn write performance
- `table_open_cache`: Cache table descriptors ‚Üí gi·∫£m file opens

### 6.2. Apply Configuration

**C√°ch 1: Edit config file**
```bash
# Linux
sudo nano /etc/mysql/my.cnf

# Windows
# Edit C:\ProgramData\MySQL\MySQL Server 8.0\my.ini
```

**C√°ch 2: Runtime (t·∫°m th·ªùi)**
```sql
SET GLOBAL max_connections = 500;
SET GLOBAL innodb_buffer_pool_size = 8589934592; -- 8GB
```

**Restart MySQL:**
```bash
# Linux
sudo systemctl restart mysql

# Windows
# Restart MySQL service t·ª´ Services
```

### 6.3. Verify MySQL Configuration

```sql
-- Ki·ªÉm tra max_connections
SHOW VARIABLES LIKE 'max_connections';

-- Ki·ªÉm tra buffer pool
SHOW VARIABLES LIKE 'innodb_buffer_pool_size';

-- Ki·ªÉm tra connections hi·ªán t·∫°i
SHOW STATUS LIKE 'Threads_connected';
SHOW STATUS LIKE 'Max_used_connections';

-- Ki·ªÉm tra slow queries
SHOW VARIABLES LIKE 'slow_query_log';
SHOW VARIABLES LIKE 'long_query_time';
```

---

## 7. MONITORING & METRICS

### 7.1. Spring Boot Actuator

**Th√™m dependency (m·ªói service):**

**File:** `pom.xml`

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>

<!-- Optional: Prometheus metrics -->
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-registry-prometheus</artifactId>
</dependency>
```

**File:** `application.properties`

```properties
# ‚úÖ Enable Actuator Endpoints
management.endpoints.web.exposure.include=health,metrics,info,prometheus
management.endpoint.health.show-details=always

# ‚úÖ Metrics Export
management.metrics.export.prometheus.enabled=true
management.metrics.tags.application=${spring.application.name}
```

**Access endpoints:**
```bash
# Health check
curl http://localhost:8005/actuator/health

# All metrics
curl http://localhost:8005/actuator/metrics

# Specific metric
curl http://localhost:8005/actuator/metrics/hikaricp.connections.active

# Prometheus format
curl http://localhost:8005/actuator/prometheus
```

### 7.2. Key Metrics C·∫ßn Monitor

**Database:**
- `hikaricp.connections.active` - S·ªë connections ƒëang d√πng
- `hikaricp.connections.idle` - S·ªë connections idle
- `hikaricp.connections.pending` - S·ªë requests ƒëang ch·ªù connection
- `hikaricp.connections.timeout` - S·ªë l·∫ßn timeout

**Application Server:**
- `tomcat.threads.busy` - S·ªë threads ƒëang busy
- `tomcat.threads.current` - T·ªïng s·ªë threads
- `tomcat.connections.active` - S·ªë connections active
- `http.server.requests` - HTTP request metrics

**Kafka:**
- `spring.kafka.consumer.records.lag` - Consumer lag
- `spring.kafka.consumer.records.consumed` - S·ªë records consumed
- `spring.kafka.producer.records.sent` - S·ªë records sent

**JVM:**
- `jvm.memory.used` - Memory ƒëang d√πng
- `jvm.memory.max` - Memory t·ªëi ƒëa
- `jvm.gc.pause` - GC pause time
- `jvm.threads.live` - S·ªë threads ƒëang ch·∫°y

### 7.3. Prometheus + Grafana Setup

**docker-compose.yml:**
```yaml
prometheus:
  image: prom/prometheus:latest
  ports:
    - "9090:9090"
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'

grafana:
  image: grafana/grafana:latest
  ports:
    - "3000:3000"
  environment:
    - GF_SECURITY_ADMIN_PASSWORD=admin
```

**prometheus.yml:**
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'order-service'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['localhost:8005']
  
  - job_name: 'notification-service'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['localhost:8009']
  
  - job_name: 'stock-service'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['localhost:8004']
```

---

## 8. TESTING & VERIFICATION

### 8.1. Load Testing v·ªõi Apache JMeter

**T·∫°o Test Plan:**

1. **Test HTTP Requests:**
   - Thread Group: 100 users, ramp-up 10s, loop 10
   - HTTP Request: GET /v1/stock/product
   - Listeners: Summary Report, Graph Results

2. **Test Kafka Throughput:**
   - T·∫°o script g·ª≠i 1000 messages
   - ƒêo th·ªùi gian x·ª≠ l√Ω

**Script test Kafka (Java):**
```java
@SpringBootTest
public class KafkaLoadTest {
    
    @Autowired
    private KafkaTemplate<String, SendNotificationRequest> kafkaTemplate;
    
    @Test
    public void testNotificationThroughput() {
        long start = System.currentTimeMillis();
        int count = 1000;
        
        for (int i = 0; i < count; i++) {
            SendNotificationRequest request = SendNotificationRequest.builder()
                .userId("user-" + i)
                .message("Test notification " + i)
                .build();
            kafkaTemplate.send("notification-topic", request);
        }
        
        long end = System.currentTimeMillis();
        System.out.println("Sent " + count + " messages in " + (end - start) + "ms");
        System.out.println("Throughput: " + (count * 1000.0 / (end - start)) + " messages/sec");
    }
}
```

### 8.2. Verify Configuration

**Checklist:**

- [ ] Kafka consumer concurrency = 10
- [ ] Database pool size = 20-30
- [ ] Tomcat max threads = 500
- [ ] MySQL max_connections = 500
- [ ] MySQL innodb_buffer_pool_size = 8GB
- [ ] Actuator endpoints enabled
- [ ] Metrics accessible

**Script verify:**
```bash
#!/bin/bash

echo "=== Checking Kafka Consumer Concurrency ==="
grep -r "setConcurrency" notification-service/src/
grep -r "setConcurrency" order-service/src/

echo "=== Checking Database Pool Size ==="
grep -r "hikari.maximum-pool-size" */src/main/resources/

echo "=== Checking Tomcat Threads ==="
grep -r "tomcat.threads.max" */src/main/resources/

echo "=== Checking MySQL Config ==="
mysql -u root -p -e "SHOW VARIABLES LIKE 'max_connections';"
mysql -u root -p -e "SHOW VARIABLES LIKE 'innodb_buffer_pool_size';"
```

---

## 9. CHECKLIST T·ªêI ∆ØU

### 9.1. Priority 1 (CRITICAL - L√†m ngay)

- [ ] **Notification Service Kafka:** TƒÉng `concurrency` t·ª´ 1 ‚Üí 10
- [ ] **Order Service Kafka:** T·∫°o KafkaConfig v·ªõi `concurrency = 10`
- [ ] **Database Pool:** TƒÉng `maximum-pool-size` t·ª´ 10 ‚Üí 20-30
- [ ] **MySQL:** TƒÉng `max_connections` t·ª´ 151 ‚Üí 500
- [ ] **MySQL:** Set `innodb_buffer_pool_size = 8GB` (ho·∫∑c 70% RAM)

### 9.2. Priority 2 (HIGH - L√†m trong tu·∫ßn)

- [ ] **Tomcat Threads:** TƒÉng `max-threads` t·ª´ 200 ‚Üí 500 (high traffic services)
- [ ] **Actuator:** Enable metrics v√† health endpoints
- [ ] **Monitoring:** Setup Prometheus + Grafana
- [ ] **Kafka:** T·ªëi ∆∞u `FETCH_MIN_BYTES` v√† `MAX_POLL_RECORDS`

### 9.3. Priority 3 (MEDIUM - L√†m khi c√≥ th·ªùi gian)

- [ ] **WebSocket:** Migrate sang RabbitMQ (external broker)
- [ ] **Connection Pool:** Fine-tune theo metrics th·ª±c t·∫ø
- [ ] **MySQL:** T·ªëi ∆∞u query indexes
- [ ] **Caching:** Th√™m Redis cache cho hot data

### 9.4. Testing & Validation

- [ ] Load test v·ªõi 1,000 concurrent users
- [ ] Monitor metrics trong 24h
- [ ] Verify kh√¥ng c√≥ connection pool exhaustion
- [ ] Verify Kafka consumer lag < 1000
- [ ] Verify response time p95 < 500ms

---

## 10. TROUBLESHOOTING

### 10.1. Connection Pool Exhaustion

**Symptoms:**
```
HikariPool - Connection is not available, request timed out after 30000ms
```

**Solutions:**
1. TƒÉng `maximum-pool-size`
2. Ki·ªÉm tra connection leaks (enable `leak-detection-threshold`)
3. TƒÉng `connection-timeout` (t·∫°m th·ªùi)
4. Ki·ªÉm tra slow queries

### 10.2. Kafka Consumer Lag

**Symptoms:**
- Messages x·ª≠ l√Ω ch·∫≠m
- Consumer lag tƒÉng li√™n t·ª•c

**Solutions:**
1. TƒÉng `concurrency` (‚â§ s·ªë partitions)
2. T·ªëi ∆∞u consumer logic (gi·∫£m processing time)
3. Scale out (th√™m consumer instances)
4. TƒÉng partitions (n·∫øu c·∫ßn)

### 10.3. High CPU Usage

**Symptoms:**
- CPU > 80%
- Response time tƒÉng

**Solutions:**
1. Ki·ªÉm tra s·ªë threads (c√≥ th·ªÉ qu√° nhi·ªÅu)
2. T·ªëi ∆∞u code (gi·∫£m CPU-intensive operations)
3. Scale out (th√™m instances)
4. Profile code ƒë·ªÉ t√¨m bottleneck

### 10.4. Out of Memory

**Symptoms:**
```
OutOfMemoryError: Java heap space
```

**Solutions:**
1. TƒÉng JVM heap size: `-Xmx4g -Xms4g`
2. Ki·ªÉm tra memory leaks
3. Gi·∫£m cache size
4. T·ªëi ∆∞u data structures

---

## 11. K·∫æT LU·∫¨N

Sau khi √°p d·ª•ng c√°c t·ªëi ∆∞u tr√™n, h·ªá th·ªëng s·∫Ω c√≥ th·ªÉ:

‚úÖ **Ch·ªãu t·∫£i:** ~5,000-10,000 concurrent users  
‚úÖ **Throughput:** 10x improvement  
‚úÖ **Latency:** Gi·∫£m ƒë√°ng k·ªÉ  
‚úÖ **Stability:** TƒÉng cao  

**L∆∞u √Ω:**
- T·ªëi ∆∞u l√† qu√° tr√¨nh li√™n t·ª•c, kh√¥ng ph·∫£i m·ªôt l·∫ßn
- Monitor metrics th∆∞·ªùng xuy√™n
- Fine-tune d·ª±a tr√™n data th·ª±c t·∫ø
- Scale out khi c·∫ßn thi·∫øt

**Next Steps:**
1. Apply Priority 1 optimizations
2. Monitor trong 1 tu·∫ßn
3. Fine-tune d·ª±a tr√™n metrics
4. Apply Priority 2 & 3

Good luck! üöÄ

